---
marp: true
theme: gaia
paginate: true
footer: '数学建模协会 2021-11-25'
style: |
  section a {
      font-size: 40px;
  }
---

<!--
_class: lead gaia
_paginate: false
-->

# 数学建模协会第八次讨论班
## ——非线性规划

### 上海交通大学数学科学学院
### 白志威
#### 2021-11-25

---
<style scoped>
section ul li {
    font-size: 50px;
}
</style>
<!-- backgroundColor: white -->
# 目录
- 1. 非线性规划介绍
- 2. 无约束非线性规划求解
- 3. 有约束非线性规划求解
![bg right:50% h:13cm](https://gitee.com/bai299/images/raw/master/%E5%8D%8F%E4%BC%9Alogo.jpg)


---
$\quad$
$\quad$
$\quad$
$\quad$
# $\quad$ $\quad$ $\quad$  1. 非线性规划介绍

---
<style scoped>
section ul li {
    font-size: 34px;
}
</style>
## 非线性规划介绍
- 如果目标函数或约束条件中包含非线性函数，就称这种规划问题为非线性规划问题。一般说来，解非线性规划要比解线性规划问题困难得多。
- 不像线性规划有单纯形法这一通用方法，非线性规划目前还没有适于各种问题的一般算法，各个方法都有自己特定的适用范围。
![bg right 99%](https://se.mathworks.com/help/examples/optim/win64/SolveConstrainedNonlinearOptimizationProblemBasedExample_01.png)


---
<style scoped>
section ul li {
    font-size: 36px;
}
</style>
## 非线性规划介绍
- 与线性规划问题不同，非线性规划问题可以有约束条件，也可以没有约束条件。非线性规划模型的一般形式描述如下:
$$
\begin{aligned}
&\min f(x), \\
&\text { s.t. } \begin{cases}g_{i}(x) \leq 0, & i=1,2, \cdots, m \\
h_{j}(x)=0, & j=1,2, \cdots, l,\end{cases}
\end{aligned}
$$
- 其中 $x=\left[x_{1}, x_{2}, \cdots, x_{n}\right]^{T} \in R^{n}$, 而 $f, g_{i}, h_{j}$ 都是定义在 $\mathbb{R}^{n}$ 上的实值函数。

---
$\quad$
$\quad$
$\quad$
$\quad$
# $\quad$ $\quad$  2. 无约束非线性规划求解

---
<style scoped>
section ul li {
    font-size: 36px;
}
</style>
## 无约束非线性规划求解
- **定理** *（无约束优化问题有局部最优解的充分条件）设 $f(x)$ 具有连 续的二阶偏导数, 点 $x^{*}$ 满足 $\nabla f\left(x^{*}\right)=0$; 并且 $\nabla^{2} f\left(x^{*}\right)$ 为正定阵, 则 $x^{*}$ 为无约束优化问题的局部最优解。*
- 该定理给出了求解无约束优化问题的理论方法, 但困难的是求解方程 
$$
\nabla f\left(x^{*}\right)=0
$$
- 对于比较复杂的函数, 常用的方法是数值解法, 如梯度下降法、牛顿法和拟牛顿法等。

---
<style scoped>
section ul li {
    font-size: 33px;
}
</style>
## 梯度下降法
- 比如我们在大山的某处位置，由于不知道怎么下山，于是决定走一步算一步，也就是在每走到一个位置的时候，求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的方向向下走一步，然后继续求解当前位置梯度，沿着负梯度方向再走一步，一直走到觉得我们已经到了山脚。
![bg right 90%](https://www.researchgate.net/profile/Alexander-Amini/publication/325142728/figure/fig1/AS:766109435326465@1559666131320/Non-convex-optimization-We-utilize-stochastic-gradient-descent-to-find-a-local-optimum.jpg)


---
<style scoped>
section ul li {
    font-size: 33px;
}
</style>
## 梯度下降法
- **例子**： 
$$
\min f(x, y) = (x-10)^2 + (y-10)^2
$$
- 给定初始点 $(x_0, y_0)$
- 按照如下方式更新位置
$$
\left(x_t, \\ y_t\right)^T = (x_{t-1}, y_{t-1})^T - \alpha \nabla f
$$
- 直到算法收敛

$\quad$ $\quad$ $\quad$ $\quad$ $\quad$![bg right 99%](https://gitee.com/bai299/images/raw/master/image-20211125092856731.png)

---
<style scoped>
section ul li {
    font-size: 33px;
}
</style>
```python
import sympy as sp
import numpy as np
def Func(x,y):
    return (x-10)**2 + (y-10)**2
def Grad_decent(func, x_init, alpha=0.1, step=100):
    i = 1
    while i <= step:
        gradient = np.array([sp.diff(func, x).subs([(x, x_init[0]),(y, x_init[1])]),
                sp.diff(func, y).subs([(x, x_init[0]),(y, x_init[1])])], dtype=float) 
        x_init -= alpha * gradient 
        print(f'迭代第{i}次[x, y]为：{x_init}')
        i = i + 1
        if np.linalg.norm(gradient) < 1e-2: #设置迭代停止条件
            break
    return x_init
x0 = np.array([20,20], dtype=float)
x, y = sp.symbols("x, y")
f=Func(x,y)
x_sol = Grad_decent(f, x0)
```
---
<style scoped>
section ul li {
    font-size: 33px;
}
</style>
## 梯度下降法
$\quad$ $\quad$ $\quad$ ![h:13cm](https://gitee.com/bai299/images/raw/master/image-20211125111702816.png)

---
$\quad$
$\quad$
$\quad$
$\quad$
# $\quad$ $\quad$  3. 有约束非线性规划求解

---
<style scoped>
section ul li {
    font-size: 36px;
}
</style>
## 罚函数法
- 罚函数法的基本思想是：
- 利用问题的目标函数和约束函数构造出带参数的所谓增广目标函数，从而把有约束非线性规划问题转化为一系列无约束非线性规划问题来求解。
- 而增广目标函数通常由两个部分构成，一部分是原问题的目标函数，另一部分是由约束函数构造出的“惩罚”项，“惩罚”项的作用是对“违规”的点进行“惩罚”。

---
<style scoped>
section ul li {
    font-size: 36px;
}
</style>
## 罚函数法
- 比较有代表性的一种罚函数法是所谓的外部罚函数法, 或称外点法, 这种方法的迭代点一般在可行域的外部移动, 随着迭代次数的增加, “惩罚”的力度也越来越大, 从而迫使迭代点向可行域靠近。
- 具体操作方式为: 根据不等式约束 $g_{i}(x) \leq 0$ 与等式约束 $\max \left\{0, g_{i}(x)\right\}=0$ 的等价性, 构造增广目标函数（也称为罚函数）
$$
T(x, M)=f(x)+M \sum_{i=1}^{m}\left[\max \left\{0, g_{i}(x)\right\}\right]+M \sum_{j=1}^{l}\left[h_{j}(x)\right]^{2}
$$

---
<style scoped>
section ul li {
    font-size: 36px;
}
</style>
## 罚函数法
$$
\left\{\begin{array}{l}
\min f(x)=x_{1}^{2}+x_{2}^{2}+8 \\
x_{1}^{2}-x_{2} \geq 0 \\
-x_{1}-x_{2}^{2}+2=0 \\
x_{1}, x_{2} \geq 0
\end{array}\right.
$$
- 最优解：$x_1 = x_2 =1$，函数值：10，增广函数为：
$$
T(x, M) = f(x) + M\max(-x_1, 0) + M\max(-x_2, 0) \\+ M\max(-x_1^2+x_2, 0)+
M(-x_1-x_2^2+2)^2
$$

---
<style scoped>
section ul li {
    font-size: 36px;
}
</style>
## 罚函数法
```python
from scipy.optimize import minimize
def func2(x, M=1e7):
    f=x[0]**2+x[1]**2+8
    g=f + M*max(-x[0],0) + M*max(-x[1],0) 
    + M*max(-x[0]**2+x[1],0) + M*(-x[0]-x[1]**2+2)**2
    return g
res=minimize(func2, np.random.rand(2)) #第 2 个参数为初值
print(res.fun,'\n',res.x) # 输出最优值、求解状态、最优解
```
```
10.001400242949867 
[1.00058428 0.99971231]
```

---
<style scoped>
section h2 {
    text-align: center;
    font-size: 100px;
}
</style>
$\quad$
$\quad$
$\quad$
## 感谢聆听！


